{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NvusujU9_5pIUbn9SZ6hMA</td>\n",
       "      <td>Stopped by to munch a burger during today's Se...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vHOeBa7aMA_na4rfS2Db5A</td>\n",
       "      <td>Yelp doesn't allow to leave 0 star review, so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hG9RTxxivb0ZXzEk4JXTXA</td>\n",
       "      <td>I find it hard to believe there are so many pe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zIVkwgahZjOneChZFUYY4g</td>\n",
       "      <td>Love this place! Almost all of their menu item...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DLczAuvMAlAnY5EeDGhTVg</td>\n",
       "      <td>Excellent customer service. I wish I could ren...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63446</th>\n",
       "      <td>OgoBp7fbXnLSKvsQb4O_tw</td>\n",
       "      <td>I really loved the food and service. I mean, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63447</th>\n",
       "      <td>Q7e8EtZMmdknDrQE7huMoQ</td>\n",
       "      <td>Their Grove location was the bomb. Delicious f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63448</th>\n",
       "      <td>zzMW6zbsFaQMjoGu2bGVdA</td>\n",
       "      <td>A nice ean BBQ joint right across from some ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63449</th>\n",
       "      <td>scgoa60EvhW2Mz7JMqLYGw</td>\n",
       "      <td>The perfect Hookah bar. I'm not sure what they...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63450</th>\n",
       "      <td>M7NAWZ9P99ucR66LrxrUOQ</td>\n",
       "      <td>Besides it being super busy often, usually, I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63451 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      NvusujU9_5pIUbn9SZ6hMA   \n",
       "1      vHOeBa7aMA_na4rfS2Db5A   \n",
       "2      hG9RTxxivb0ZXzEk4JXTXA   \n",
       "3      zIVkwgahZjOneChZFUYY4g   \n",
       "4      DLczAuvMAlAnY5EeDGhTVg   \n",
       "...                       ...   \n",
       "63446  OgoBp7fbXnLSKvsQb4O_tw   \n",
       "63447  Q7e8EtZMmdknDrQE7huMoQ   \n",
       "63448  zzMW6zbsFaQMjoGu2bGVdA   \n",
       "63449  scgoa60EvhW2Mz7JMqLYGw   \n",
       "63450  M7NAWZ9P99ucR66LrxrUOQ   \n",
       "\n",
       "                                                    text  positive  neutral  \\\n",
       "0      Stopped by to munch a burger during today's Se...         1        0   \n",
       "1      Yelp doesn't allow to leave 0 star review, so ...         0        0   \n",
       "2      I find it hard to believe there are so many pe...         0        0   \n",
       "3      Love this place! Almost all of their menu item...         1        0   \n",
       "4      Excellent customer service. I wish I could ren...         1        0   \n",
       "...                                                  ...       ...      ...   \n",
       "63446  I really loved the food and service. I mean, t...         1        0   \n",
       "63447  Their Grove location was the bomb. Delicious f...         1        0   \n",
       "63448  A nice ean BBQ joint right across from some ne...         1        0   \n",
       "63449  The perfect Hookah bar. I'm not sure what they...         1        0   \n",
       "63450  Besides it being super busy often, usually, I ...         0        1   \n",
       "\n",
       "       negative  \n",
       "0             0  \n",
       "1             1  \n",
       "2             1  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "63446         0  \n",
       "63447         0  \n",
       "63448         0  \n",
       "63449         0  \n",
       "63450         0  \n",
       "\n",
       "[63451 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus data\n",
    "\n",
    "corpus = open('corpus.txt', 'r').read()\n",
    "corpus = corpus.split('\\n')\n",
    "corpus = corpus[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "stop munch burger today seahawk saint game place unsurprisingli pack good reason burger order fantast sat right next door get chanc get six feet place soon got meal place start get busier busier work way world shortest peopl maze get guess mean first person hear place go back might go say lunch tuesday less busi\n",
      "--------------------------------------------------\n",
      "yelp allow leav star review see one star wife move ny south california contact differ move compani sent initi email unit van line soon got email back virtual survey confirm ladi virtual survey meticul profession screen whole apart minut one contact sent second email almost month ask everyth ok get quot need mention compani sent quot less hour sinc one repli til today call direct phone left messag answer machin one call back hope everyth ok one hurt see reason explan avoid unprofession\n",
      "--------------------------------------------------\n",
      "find hard believ mani peopl low standard come daili bread ripoff st loui bread co panera bread mind eaten multipl time daili bread yet enjoy experi food best sandwich basic lack real flavor also tri bbq chicken pizza okay certainli steer anyon away place base food food basic simpl bad also noth great biggest qualm daili bread servic major worker seem utterli uninterest help line peopl one person regist sens urgenc help custom line wait good minut put order receiv cup get drink look clean place sit eventu settl tabl top least dirti tabl avail plenti open spot appear though lot trash left behind anyon bu tabl anticip peopl respons clean appear though tabl effect clean also trash floor address well order number call order halfway complet ask sandwich basic ignor girl put two order anoth minut rest order came began eat tri item state okay problem food unabl make lack organ slow speed serv cleanli restaur attempt write daili bread return rather go st loui bread co get level food clean environ time set\n",
      "--------------------------------------------------\n",
      "love place almost menu item made vegan super knowledg pad thai tofu veggi cashew perfect avocado egg roll amaz favorit\n",
      "--------------------------------------------------\n",
      "excel custom servic wish could rent often great experi vers car rental servic rent\n"
     ]
    }
   ],
   "source": [
    "# see contents of corpus\n",
    "\n",
    "for review in corpus[:5]:\n",
    "    print('-' * 50)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus data with stopwords\n",
    "\n",
    "corpus_stopwords = open('corpus_stopwords.txt', 'r').read()\n",
    "corpus_stopwords = corpus_stopwords.split('\\n')\n",
    "corpus_stopwords = corpus_stopwords[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "stop by to munch a burger dure today s seahawk saint game and the place wa unsurprisingli pack for good reason too the haven burger i order wa fantast i sat right next to the door so i didn t get the chanc to get more than six feet into the place as soon as i got my meal the place start get busier and busier until i had to work my way through the world s shortest peopl maze to get out guess that mean i wasn t the first person to hear about the place i will be go back to the haven might go for say lunch on a tuesday so it s less busi\n",
      "--------------------------------------------------\n",
      "yelp doesn t allow to leav star review so that is whi you see one star me and my wife are move from ny to south california and we contact differ move compani i sent my initi email to unit van line on and soon i got email back with virtual survey confirm for the ladi that did virtual survey wa veri meticul and profession and we screen through the whole apart in minut no one contact me after that so i sent a second email almost a month after on ask if everyth is ok and if i can get the quot i don t need to mention that other compani sent their quot in less than hour sinc no one repli til today i call the direct phone and i left the messag on the answer machin no one call me back i hope everyth is ok and that no one is hurt becaus i don t see ani other reason explan whi they are avoid me veri veri unprofession\n",
      "--------------------------------------------------\n",
      "i find it hard to believ there are so mani peopl with such low standard when it come to the daili bread it is a ripoff of st loui bread co panera bread which in of itself i don t mind at all i have eaten multipl time at the daili bread and have yet to have an enjoy experi the food is so so at best their sandwich are basic and lack ani real flavor i also tri the bbq chicken pizza and it wa okay i certainli wouldn t steer anyon away from thi place base on the food the food is basic and simpl not bad but also noth great my biggest qualm with the daili bread is the servic the major of the worker seem utterli uninterest in help out there wa a line of about peopl and one person at the regist there wa no sens of urgenc to help ani of the custom in line we wait for a good minut to put an order in after receiv our cup to get our drink we look for a clean place to sit and eventu settl on a tabl top that wa the least dirti of the tabl avail there were plenti of open spot but it just appear as though there wa lot of trash left behind doe anyon bu these tabl i anticip that peopl are respons for clean up after themselv but it appear as though the tabl hadn t been effect clean in a while there wa also some trash on the floor that hadn t been address as well the order number wa call and the order wa onli halfway complet when we ask which sandwich wa our we were basic ignor as the girl put up two more order after anoth few minut the rest of the order came up and we began to eat i tri both item and as i state befor both were okay the problem is that the food wa just unabl to make up for the lack of organ the slow speed at which we were serv or the cleanli of the restaur after attempt i m write the daili bread off and won t return i d rather go to st loui bread co get the same level of food but in a clean environ and a time set\n",
      "--------------------------------------------------\n",
      "love thi place almost all of their menu item can be made vegan they are super knowledg the pad thai with tofu veggi and cashew is perfect the avocado egg roll are amaz too a favorit\n",
      "--------------------------------------------------\n",
      "excel custom servic i wish i could rent from there more often had such a great experi vers other car rental servic rent if you can\n"
     ]
    }
   ],
   "source": [
    "# see contents of corpus with stopwords\n",
    "\n",
    "for review in corpus_stopwords[:5]:\n",
    "    print('-' * 50)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec vectors\n",
    "\n",
    "wv = KeyedVectors.load(\"reviews_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(embeddings, text, sequence_len, strategy=None):\n",
    "    '''\n",
    "    Function to convert text to word embeddings\n",
    "    '''\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True\n",
    "        finally:\n",
    "            i += 1\n",
    "    for _ in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    if strategy == 'mean':\n",
    "        vec = np.mean(vec, axis=0)\n",
    "    elif strategy == 'max':\n",
    "        vec = np.max(vec, axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 63451\n",
      "Minimum number of words: 1\n",
      "Maximum number of words: 488\n",
      "Average number of words: 53.2330302122898\n",
      "Standard deviation of words: 48.55774698364295\n",
      "Mode of words: ModeResult(mode=13, count=1267)\n"
     ]
    }
   ],
   "source": [
    "# corpus statistics\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "\n",
    "print('Number of reviews:', len(corpus))\n",
    "print('Minimum number of words:', np.min(lens))\n",
    "print('Maximum number of words:', np.max(lens))\n",
    "print('Average number of words:', np.mean(lens))\n",
    "print('Standard deviation of words:', np.std(lens))\n",
    "print('Mode of words:', stats.mode(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "\n",
    "embeddings_corpus = []\n",
    "word_limit = 50\n",
    "for review in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(wv, review, word_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizers\n",
    "\n",
    "bag_of_wors = CountVectorizer()\n",
    "one_hot = CountVectorizer(binary=True)\n",
    "n_grams = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n",
    "tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 7500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a vectorizer to the corpus (stopwords included if using N-grams, for negation tokens)\n",
    "\n",
    "vectorizer = None\n",
    "\n",
    "X = None\n",
    "if vectorizer is None:\n",
    "    X = np.array(embeddings_corpus)\n",
    "elif vectorizer == n_grams:\n",
    "    X = vectorizer.fit_transform(corpus_stopwords).toarray()\n",
    "else:\n",
    "    X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target labels\n",
    "\n",
    "y = review_df[['positive', 'neutral', 'negative']]\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50760, 7500) (50760, 3)\n",
      "(12691, 7500) (12691, 3)\n",
      "\n",
      "Label distribution in the training set:\n",
      "positive  neutral  negative\n",
      "1         0        0           34139\n",
      "0         0        1           11713\n",
      "          1        0            4908\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution in the test set:\n",
      "positive  neutral  negative\n",
      "1         0        0           8541\n",
      "0         0        1           2946\n",
      "          1        0           1204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102417, 7500) (102417, 3)\n",
      "\n",
      "Label distribution after oversampling:\n",
      "positive  neutral  negative\n",
      "0         0        1           34139\n",
      "          1        0           34139\n",
      "1         0        0           34139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# oversampling to balance the classes\n",
    "\n",
    "oversampler = RandomOverSampler()\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train.to_numpy())\n",
    "\n",
    "print(X_train_resampled.shape, y_train_resampled.shape)\n",
    "\n",
    "print(\"\\nLabel distribution after oversampling:\")\n",
    "print(pd.DataFrame(y_train_resampled, columns=['positive', 'neutral', 'negative']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "\n",
    "nayve_bayes = MultinomialNB()\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "perceptron = Perceptron(tol=1e-3, random_state=0)\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier\n",
    "\n",
    "clf = MultiOutputClassifier(logistic_regression)\n",
    "clf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "print(y_pred_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "cv_scores = cross_validate(clf, X, y, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], cv=5)\n",
    "\n",
    "# Memory Error popping up here (too much dimensionality???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER sentiment analysis\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "res = []\n",
    "y_pred_vader = []\n",
    "for review in review_df['text']:\n",
    "    sentiment = vader.polarity_scores(review)\n",
    "    res.append(sentiment)\n",
    "    stronger_value = max(sentiment['pos'], sentiment['neu'], sentiment['neg'])\n",
    "    if sentiment['pos'] == stronger_value:\n",
    "        y_pred_vader.append([1, 0, 0])\n",
    "    elif sentiment['neu'] == stronger_value:\n",
    "        y_pred_vader.append([0, 1, 0])\n",
    "    elif sentiment['neg'] == stronger_value:\n",
    "        y_pred_vader.append([0, 0, 1])\n",
    "        \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred):\n",
    "    '''\n",
    "    Evaluate the performance of a multi-label classifier\n",
    "    '''\n",
    "    multilabel_cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(\"Multilabel Confusion Matrix:\")\n",
    "    print(multilabel_cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics - classifier\n",
    "\n",
    "evaluate_model(y_test, y_pred_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics - VADER\n",
    "\n",
    "y_true = review_df[['positive', 'neutral', 'negative']]\n",
    "y_true = y_true.values\n",
    "\n",
    "evaluate_model(y_true, y_pred_vader)\n",
    "\n",
    "# Note: values are like this probably because of criteria applied when building y_pred_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with VADER sentiment scores\n",
    "\n",
    "vader_df = pd.DataFrame(res)\n",
    "vader_df.columns = ['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']\n",
    "vader_df = pd.concat([review_df, vader_df], axis=1)\n",
    "\n",
    "vader_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VADER compound scores by sentiment\n",
    "\n",
    "positive_scores = vader_df[vader_df['positive'] == 1]['vader_compound']\n",
    "neutral_scores = vader_df[vader_df['neutral'] == 1]['vader_compound']\n",
    "negative_scores = vader_df[vader_df['negative'] == 1]['vader_compound']\n",
    "all_scores = pd.concat([positive_scores, neutral_scores, negative_scores])\n",
    "\n",
    "sentiments = ['positive'] * len(positive_scores) + ['neutral'] * len(neutral_scores) + ['negative'] * len(negative_scores)\n",
    "sentiment_scores = pd.DataFrame({'sentiment': sentiments, 'vader_compound': all_scores})\n",
    "\n",
    "sns.barplot(data=sentiment_scores, x='sentiment', y='vader_compound')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('VADER Compound Score')\n",
    "plt.title('VADER Compound Score by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation results\n",
    "\n",
    "cv_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
